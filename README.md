# Reinforcement Learning Driven Dota 2 Bot Development

![DOTA2](https://github.com/soumyendra98/DQN-Game-Bot/blob/main/images/download.gif)


## Team Members:
- Soumyendra Shrivastava (016670121)
- Siddhant Sancheti (016710421)


## Summary üìù

Our project ambitiously aims to develop a bot for the intricate and competitive MOBA game, Dota 2. Utilizing the principles of Reinforcement Learning (RL) and Deep Q-Networks (DQN), we aspire to create an AI-driven bot capable of making sophisticated real-time decisions, setting new standards in AI for gaming.

## Background and Motivation üåê

Dota 2, as a complex and strategic MOBA game, offers a unique challenge in AI and RL. The game's multifaceted environment presents numerous opportunities for AI to make real-time, strategic decisions, a scenario that is not only limited to gaming but extends to various real-world applications.

## Why This Project? üîç

- Exploring AI's Potential: Our project is a continuation of the legacy set by OpenAI Five, pushing the boundaries of AI in strategic decision-making.
- Innovative Approach: Leveraging DQN, we aim to enhance our bot's learning process, allowing it to adapt to the dynamic nature of Dota 2.
- Real-World Applications: Success in this project could pave the way for implementing similar AI strategies in real-life scenarios demanding quick and strategic decision-making.

## Current Approaches üîÑ

Current methodologies primarily involve RL and DQN for training bots to navigate Dota 2's multifaceted environment. While significant progress has been made, the quest for a bot that can match or surpass human proficiency remains open, underscoring the need for ongoing innovation.

## Project Goal üéØ

To create a Dota 2 bot that exemplifies high-level strategic play and decision-making using DQN.

## Diagrams
A. Architecture Diagram

- The architecture of Dota 2 bot with key components and their interactions:
  
  - Agent: Central component representing the reinforcement learning agent.
    
  - Neural Network: A Deep Q-Network (DQN) & Policy Gradient network used for training and action prediction.
    
  - Action Space: Defines the possible actions the agent can take in the Dota 2 environment.
    
  - Environment: The Dota 2 game environment that interacts with the agent.
    
  - Replay Buffer: Stores experiences for training the neural network.

    <img width="675" alt="Screen Shot 2023-12-10 at 8 43 59 PM" src="https://github.com/soumyendra98/DQN-Game-Bot/assets/47080427/fda40e22-2749-404c-a556-0f9b9ce886b4">
    

B. RL Process: 

- This diagram illustrates the core reinforcement learning loop in your Dota 2 bot project, including interactions between the agent, environment, and learning components.

  <img width="725" alt="Screen Shot 2023-12-10 at 8 44 07 PM" src="https://github.com/soumyendra98/DQN-Game-Bot/assets/47080427/9295971b-dcb2-42db-b2a8-615ef173e316">


C. Action-Selection Flow: 

- This diagram outlines how actions are selected based on the policy network's output and the action space.

<img width="691" alt="Screen Shot 2023-12-10 at 8 44 17 PM" src="https://github.com/soumyendra98/DQN-Game-Bot/assets/47080427/ba44e608-91a5-4b35-ab09-524cd677abe9">


## Scope üìà

### In Scope

- Development and training of the Dota 2 bot using DQN.
- Iterative evaluation and refinement of the bot‚Äôs performance.

### Out of Scope

- Utilizing ML algorithms beyond the scope of DQN.
- Commercial deployment of the bot.

## Deliverables üì¶

A comprehensive Python-based implementation of the Dota 2 bot, encompassing all RL elements: Agent, Environment, Action, Policy, and Reward systems.

## Risks and Rewards ‚öñÔ∏è

### Risks
- Computational Resources: The complexity of Dota 2 might require extensive computational power and time.
- Game Mechanics: Unpredictable challenges in game mechanics could pose hurdles in the learning process.

### Rewards

- AI in Gaming: A breakthrough would significantly advance the use of AI in gaming.
- Broader Applications: The methodologies could be replicated in other areas requiring strategic real-time decision-making.

## Implementation and Results üöÄ

### Methodology

We employed a blend of Python, RL, and DQN in our implementation. The project was structured as follows:

- Environment Setup: Customized Dota 2 environment for the bot.
  
- Agent Development: Utilizing RL principles to develop the decision-making agent.
  
- Training Process: Leveraging DQN for training the bot.
  
- Iterative Testing: Continuous testing and refinement of the bot's strategies.
  
## Results and Contributions

- Bot Proficiency: Achieved a high level of gameplay proficiency in Dota 2.

- Innovative Solutions: Developed novel solutions to enhance strategic gameplay.

- AI Learning Efficiency: Demonstrated the effectiveness of DQN in rapid learning and adaptation.

## Conclusions and Future Work üîÆ

The project successfully demonstrated the potential of DQN in developing a proficient Dota 2 bot. Our approach has opened avenues for further exploration in AI's role in gaming and other strategic, real-time decision-making domains.

## Future Directions

- Scalability: Exploring the scalability of our approach to other complex gaming environments.

- Algorithm Enhancement: Continual refinement of DQN and RL methodologies for improved efficiency.

- Real-world Applications: Applying the insights gained to real-world scenarios requiring strategic decision-making.
